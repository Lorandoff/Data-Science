{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport nltk\nimport string\nimport tqdm\n\nnltk.download('punkt')\nnltk.download('stopwords')\nprint(tf.__version__)","metadata":{"id":"3sUP38yTBJWz","outputId":"7cefe636-f044-4b71-86fc-351cc560d669","execution":{"iopub.status.busy":"2021-10-06T01:09:31.407894Z","iopub.execute_input":"2021-10-06T01:09:31.408251Z","iopub.status.idle":"2021-10-06T01:10:17.480992Z","shell.execute_reply.started":"2021-10-06T01:09:31.408153Z","shell.execute_reply":"2021-10-06T01:10:17.479735Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"## Loading data, EDA, data preparation.","metadata":{"id":"sQA18tJtBJW5"}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/genre-classification-dataset-imdb/Genre Classification Dataset/train_data.txt', sep=\":::\", header=None, engine='python')\ntrain_df.columns = ['id', 'title', 'genre', 'description']\ntrain_df.head()","metadata":{"id":"1FbwlUrBBJW6","outputId":"ff9d81be-dede-4152-ca1a-8a6deecbd3df","execution":{"iopub.status.busy":"2021-10-06T01:10:17.482312Z","iopub.execute_input":"2021-10-06T01:10:17.482528Z","iopub.status.idle":"2021-10-06T01:10:18.234014Z","shell.execute_reply.started":"2021-10-06T01:10:17.482506Z","shell.execute_reply":"2021-10-06T01:10:18.233026Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"test_df = pd.read_csv('../input/genre-classification-dataset-imdb/Genre Classification Dataset/test_data_solution.txt', sep=\":::\", header=None, engine='python')\ntest_df.columns = ['id', 'title', 'genre', 'description']\ntest_df.head()","metadata":{"id":"ZerE5vWEBJW6","outputId":"0fc838fb-bff8-4066-e493-120c998de4c9","execution":{"iopub.status.busy":"2021-10-06T01:10:18.235252Z","iopub.execute_input":"2021-10-06T01:10:18.235632Z","iopub.status.idle":"2021-10-06T01:10:19.233036Z","shell.execute_reply.started":"2021-10-06T01:10:18.235612Z","shell.execute_reply":"2021-10-06T01:10:19.231913Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data_type = {0: 'train_data', 1: 'test_data'}\n\nfor i, df in enumerate([train_df, test_df]):\n    print(f'Dataset \"{data_type[i]}\" info:')\n    print(df.info())\n    print('Number of unique genres: ', df['genre'].nunique())\n    print('=====================================\\n')\n\nplt.figure(figsize=(15,5))\nfor i, df in enumerate([train_df, test_df]):\n    data = df.groupby('genre').count()\n    plt.subplot(1, 2, i + 1)\n    plt.bar(data.index, data['id'])\n    plt.xlabel('Genre')\n    plt.ylabel('Number of entries')\n    plt.title(f'Genres distribution {data_type[i]}')\n    plt.xticks(rotation=90)\nplt.show()\ndel data_type\nNUMBER_OF_GENRES = train_df['genre'].nunique()","metadata":{"id":"0LViFxsjBJW6","outputId":"095f4ca4-ec71-455d-b4a0-788156518cb4","execution":{"iopub.status.busy":"2021-10-06T01:10:19.234376Z","iopub.execute_input":"2021-10-06T01:10:19.234587Z","iopub.status.idle":"2021-10-06T01:10:19.790397Z","shell.execute_reply.started":"2021-10-06T01:10:19.234566Z","shell.execute_reply":"2021-10-06T01:10:19.789139Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"stop_words = set(nltk.corpus.stopwords.words('english'))\n\ndef tokenize_text(raw_text: str):\n    tokenized_str = nltk.word_tokenize(raw_text)\n    tokens = [i.lower() for i in tokenized_str if (i not in string.punctuation) and (i not in stop_words)]\n    # filtered_tokens = [i for i in tokens if ( i not in stop_words )]\n    # lemmatized_tokens = [morph.parse(i)[0].normal_form for i in tokens]\n    return tokens\n\ntrain_df['tokenized'] = train_df.description.apply(tokenize_text)\ntest_df['tokenized'] = test_df.description.apply(tokenize_text)\nprint('Max description length in tokens: ', train_df.tokenized.apply(len).max())","metadata":{"id":"AA12DREABJW7","outputId":"24f0b264-7307-4f40-981a-fec8b9d59eb9","execution":{"iopub.status.busy":"2021-10-06T01:10:19.791842Z","iopub.execute_input":"2021-10-06T01:10:19.792097Z","iopub.status.idle":"2021-10-06T01:11:39.466320Z","shell.execute_reply.started":"2021-10-06T01:10:19.792075Z","shell.execute_reply":"2021-10-06T01:11:39.465591Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"plt.hist(train_df.tokenized.apply(len), bins=30, log=True)\nplt.title('Distribution of the number of tokens in sequences')\nplt.xlabel('Length of sequence')\nplt.show()","metadata":{"id":"V4dFaY8PBJW7","outputId":"840b8de2-2d0a-42c2-807b-8a549616ef83","execution":{"iopub.status.busy":"2021-10-06T01:11:39.467440Z","iopub.execute_input":"2021-10-06T01:11:39.467653Z","iopub.status.idle":"2021-10-06T01:11:40.024228Z","shell.execute_reply.started":"2021-10-06T01:11:39.467626Z","shell.execute_reply":"2021-10-06T01:11:40.023488Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"temp = train_df.tokenized.apply(len)\n#print('Number of entries with description length > 512: ', temp[temp > 512].count())","metadata":{"id":"HiOsKlV7BJW8","outputId":"b6158fe2-307b-453a-9396-8c0ec79c5564","execution":{"iopub.status.busy":"2021-10-06T01:11:40.025445Z","iopub.execute_input":"2021-10-06T01:11:40.025728Z","iopub.status.idle":"2021-10-06T01:11:40.061905Z","shell.execute_reply.started":"2021-10-06T01:11:40.025699Z","shell.execute_reply":"2021-10-06T01:11:40.061416Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"words_dict = {'<PAD>': 0}\nwords_dict['<UNK>'] = 1\n\nindex = 2\nfor seq in tqdm.tqdm(train_df['tokenized']):\n    for token in seq:\n        if token not in words_dict:\n            words_dict[token] = index\n            index += 1\nprint('\\nVocabulary length: ', index)\nwords_dict","metadata":{"id":"P4MKw1PMBJW8","outputId":"c7774051-e9e8-49b7-e932-857bd74e3bf1","execution":{"iopub.status.busy":"2021-10-06T01:15:57.913924Z","iopub.execute_input":"2021-10-06T01:15:57.914360Z","iopub.status.idle":"2021-10-06T01:15:58.747110Z","shell.execute_reply.started":"2021-10-06T01:15:57.914336Z","shell.execute_reply":"2021-10-06T01:15:58.746102Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"inverse_words_dict = {index: token for token, index in words_dict.items()}","metadata":{"id":"8cWvoCFlBJW9","execution":{"iopub.status.busy":"2021-10-06T01:11:40.739706Z","iopub.execute_input":"2021-10-06T01:11:40.739911Z","iopub.status.idle":"2021-10-06T01:11:40.769135Z","shell.execute_reply.started":"2021-10-06T01:11:40.739890Z","shell.execute_reply":"2021-10-06T01:11:40.768347Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"words_dict[\"<UNK>\"]\nwords_dict.get(\"student\")","metadata":{"execution":{"iopub.status.busy":"2021-10-06T01:11:40.770030Z","iopub.execute_input":"2021-10-06T01:11:40.770243Z","iopub.status.idle":"2021-10-06T01:11:40.775516Z","shell.execute_reply.started":"2021-10-06T01:11:40.770222Z","shell.execute_reply":"2021-10-06T01:11:40.774716Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def decode_text(text):\n    return ' '.join([inverse_words_dict.get(i, '?') for i in text])\n\n\ndef encode_text(text):\n    words = tokenize_text(text)\n    idxs = [words_dict.get(word, words_dict['<UNK>']) for word in words]\n    return idxs","metadata":{"id":"1qrvFDR1BJW9","execution":{"iopub.status.busy":"2021-10-06T01:11:40.777034Z","iopub.execute_input":"2021-10-06T01:11:40.777568Z","iopub.status.idle":"2021-10-06T01:11:40.787354Z","shell.execute_reply.started":"2021-10-06T01:11:40.777531Z","shell.execute_reply":"2021-10-06T01:11:40.786599Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"sample_text = train_df['description'][4]\nprint(sample_text, '\\n')\nprint(encode_text(sample_text), '\\n')\nprint(decode_text(encode_text(sample_text)))\ndel sample_text","metadata":{"id":"2hKugqIpBJW9","outputId":"885527a6-5c92-4286-b5fb-213469298c64","execution":{"iopub.status.busy":"2021-10-06T01:11:40.788469Z","iopub.execute_input":"2021-10-06T01:11:40.788923Z","iopub.status.idle":"2021-10-06T01:11:40.802364Z","shell.execute_reply.started":"2021-10-06T01:11:40.788890Z","shell.execute_reply":"2021-10-06T01:11:40.801672Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"genres_dict = {}\nindex = 0\nfor gen in train_df.genre.unique():\n    genres_dict[gen] = index\n    index += 1\nreversed_genres_dict = {index: gen for gen, index in genres_dict.items()} # for later use during model evaluation\n\ny_train =  train_df['genre'].map(genres_dict).values\ny_test = test_df['genre'].map(genres_dict).values","metadata":{"id":"1c3DpvUaBJW9","execution":{"iopub.status.busy":"2021-10-06T01:11:40.803153Z","iopub.execute_input":"2021-10-06T01:11:40.803510Z","iopub.status.idle":"2021-10-06T01:11:40.829518Z","shell.execute_reply.started":"2021-10-06T01:11:40.803486Z","shell.execute_reply":"2021-10-06T01:11:40.828370Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"### Создадим тренировочную и тестовую выборки","metadata":{"id":"cHeFwPoABfYU"}},{"cell_type":"code","source":"x_train = train_df['tokenized'].apply(lambda x: [words_dict.get(i, words_dict['<UNK>']) for i in x]).values\nx_test = test_df['tokenized'].apply(lambda x: [words_dict.get(i, words_dict['<UNK>']) for i in x]).values\n\nMAX_SEQ_LEN = 256 \nVOCAB_SIZE = len(words_dict)\n\nx_train = tf.keras.preprocessing.sequence.pad_sequences(\n    x_train,\n    value=words_dict[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)\n\nx_test = tf.keras.preprocessing.sequence.pad_sequences(\n    x_test,\n    value=words_dict[\"<PAD>\"],\n    padding='post',\n    maxlen=MAX_SEQ_LEN)","metadata":{"id":"qH27nUqOBJW-","execution":{"iopub.status.busy":"2021-10-06T01:11:40.830688Z","iopub.execute_input":"2021-10-06T01:11:40.830912Z","iopub.status.idle":"2021-10-06T01:11:44.283750Z","shell.execute_reply.started":"2021-10-06T01:11:40.830891Z","shell.execute_reply":"2021-10-06T01:11:44.282618Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedShuffleSplit\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\ntrain_indices, val_indices = next(sss.split(x_test, y_test))\nx_val, y_val = x_test[val_indices], y_test[val_indices]\nx_test, y_test = x_test[train_indices], y_test[train_indices]\n\nprint('TRAIN DATA SHAPE: ', x_train.shape, y_train.shape)\nprint('VALIDATION DATA SHAPE: ', x_val.shape, y_val.shape)\nprint('TEST DATA SHAPE: ', x_test.shape, y_test.shape)","metadata":{"id":"1RCT6PPbBJW-","outputId":"91221fc8-bb82-4b7d-9f13-19228c2f8895","execution":{"iopub.status.busy":"2021-10-06T01:11:44.284910Z","iopub.execute_input":"2021-10-06T01:11:44.285123Z","iopub.status.idle":"2021-10-06T01:11:44.333028Z","shell.execute_reply.started":"2021-10-06T01:11:44.285101Z","shell.execute_reply":"2021-10-06T01:11:44.331955Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"data_type = {0: 'train_data', 1: 'test_data', 2: 'val_data'}\nplt.figure(figsize=(18,5))\nfor i, data in enumerate([y_train, y_test, y_val]):\n    plt.subplot(1, 3, i + 1)\n    plt.hist(data)\n    plt.xlabel('Genre code')\n    plt.ylabel('Number of entries')\n    plt.title(f'Genres distribution {data_type[i]}')\n    plt.xticks(rotation=90)\nplt.show()","metadata":{"id":"bEKI8yigBJW-","outputId":"c05f8473-756e-4b0f-994a-4c3fe59b011d","execution":{"iopub.status.busy":"2021-10-06T01:11:44.334040Z","iopub.execute_input":"2021-10-06T01:11:44.334269Z","iopub.status.idle":"2021-10-06T01:11:44.702681Z","shell.execute_reply.started":"2021-10-06T01:11:44.334248Z","shell.execute_reply":"2021-10-06T01:11:44.701747Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Создадим и обучим модель","metadata":{"id":"qeeI9LAwBJW_"}},{"cell_type":"code","source":"EMB_SIZE = 16\n\nmodel = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE, EMB_SIZE, input_length=MAX_SEQ_LEN, mask_zero=True),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=True, dropout=0.5, recurrent_dropout=0.5), \n        # merge_mode='sum'\n    ),\n    tf.keras.layers.Bidirectional(\n        tf.keras.layers.LSTM(EMB_SIZE, return_sequences=False, dropout=0.5, recurrent_dropout=0.5)\n    ),\n    # tf.keras.layers.Dropout(0.5),\n    tf.keras.layers.Dense(EMB_SIZE*2, activation='relu'),\n    tf.keras.layers.Dense(NUMBER_OF_GENRES, activation=tf.nn.sigmoid),\n])\n\nmodel.summary()\n\nloss = tf.losses.SparseCategoricalCrossentropy()\noptimizer = tf.optimizers.Adam(learning_rate=0.001)\n# metric = tf.keras.metrics.SparseCategoricalAccuracy()\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)\n\nmodel.compile(optimizer=optimizer, loss=loss, metrics=['sparse_categorical_accuracy'])","metadata":{"id":"vQ8WzXZNBJW_","outputId":"c4df3b1e-d00a-4b31-dba7-59c1a6ad2d31","execution":{"iopub.status.busy":"2021-10-06T01:11:44.703684Z","iopub.execute_input":"2021-10-06T01:11:44.703890Z","iopub.status.idle":"2021-10-06T01:11:45.268495Z","shell.execute_reply.started":"2021-10-06T01:11:44.703870Z","shell.execute_reply":"2021-10-06T01:11:45.267467Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"epochs_counter = 0\n\n!rm -r logs","metadata":{"id":"TISgmZJRGr62","execution":{"iopub.status.busy":"2021-10-06T01:11:45.269550Z","iopub.execute_input":"2021-10-06T01:11:45.269763Z","iopub.status.idle":"2021-10-06T01:11:45.567640Z","shell.execute_reply.started":"2021-10-06T01:11:45.269741Z","shell.execute_reply":"2021-10-06T01:11:45.566325Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 20\nBATCH_SIZE = 512\n\nmodel.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS + epochs_counter, \n          callbacks=[tensorboard_callback], validation_data=(x_val, y_val), \n          initial_epoch=epochs_counter, verbose=1)\nepochs_counter += EPOCHS","metadata":{"id":"oYpdk6B26FNx","outputId":"e4d3c2e7-f02a-4b2c-c05f-a2827239d3e0","execution":{"iopub.status.busy":"2021-10-06T01:11:45.569069Z","iopub.execute_input":"2021-10-06T01:11:45.569424Z","iopub.status.idle":"2021-10-06T01:15:09.387389Z","shell.execute_reply.started":"2021-10-06T01:11:45.569380Z","shell.execute_reply":"2021-10-06T01:15:09.385859Z"},"trusted":true},"execution_count":19,"outputs":[]}]}